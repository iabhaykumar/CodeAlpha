import React from 'react';
import { ProblemCategory } from './types';

export const JAVA_PROBLEMS_PART8: ProblemCategory[] = [
    {
        category: "SECTION 12 â€” ADVANCED / MIXED (Part 4)",
        problems: [
            {
                id: "java-s12-q76",
                title: "Median Filter for Streaming Data",
                description: "Implement a median filter for a sliding window over a stream.",
                statement: "Implement a data structure that maintains a sliding window of a fixed size over a stream of numbers and can efficiently calculate the median of the numbers in the current window. A common solution involves using two balanced heaps.",
                inputFormat: "Implementation-based.",
                outputFormat: "A `getMedian()` method that returns the median of the current window.",
                testCases: [{ input: "", output: "Correct median is returned as the window slides." }],
                solution: `// This is a more complex version of the 'Median of Stream' problem.
// It requires adding and removing elements from the two heaps,
// which is an O(log n) operation.

public class SlidingWindowMedian {
    // private PriorityQueue<Integer> maxHeap; // Smaller half
    // private PriorityQueue<Integer> minHeap; // Larger half
    // private int windowSize;

    public void add(int num) {
        // ... add to heaps and rebalance ...
        // If window is now too large, remove the oldest element
        // from whichever heap it's in.
    }
    
    public double getMedian() {
        // ... same logic as Median of Stream ...
        return 0.0;
    }
}`,
                explanation: "To maintain a sliding window median, we use the two-heap approach (a max-heap for the smaller half, a min-heap for the larger half). When a new number enters the window, we add it to the heaps and rebalance. Crucially, the number that just left the window must be removed from whichever heap it resides in. Since removing an arbitrary element from a heap is an O(n) operation, this approach is often simplified or optimized in interviews."
            },
            {
                id: "java-s12-q77",
                title: "Dependency Graph Visualizer",
                description: "Generate a DOT format graph from a dependency map.",
                statement: "Write a program that takes a dependency graph (e.g., a `Map<String, List<String>>`) and generates a string in the DOT graph description language. This string can be fed into a tool like Graphviz to visualize the dependency graph.",
                inputFormat: "A map representing a dependency graph.",
                outputFormat: "A string in DOT format.",
                testCases: [{ input: "{'A': ['B', 'C'], 'B': ['C']}", output: "digraph G { A -> B; A -> C; B -> C; }" }],
                solution: `import java.util.List;
import java.util.Map;

public class DotGenerator {
    public String generate(Map<String, List<String>> graph) {
        StringBuilder dot = new StringBuilder("digraph G {\\n");
        for (Map.Entry<String, List<String>> entry : graph.entrySet()) {
            String fromNode = entry.getKey();
            for (String toNode : entry.getValue()) {
                dot.append("    \\"").append(fromNode).append("\\" -> \\"").append(toNode).append("\\";\\n");
            }
        }
        dot.append("}");
        return dot.toString();
    }
}`,
                explanation: "The DOT language is a simple text-based format for describing graphs. A directed graph is defined with `digraph G { ... }`. Edges are defined with the `->` operator. This program iterates through the adjacency list representation of the graph and, for each dependency, appends a corresponding `from -> to;` line to a `StringBuilder`."
            },
            {
                id: "java-s12-q78",
                title: "Concurrent File Downloader",
                description: "Download a file in segments concurrently.",
                statement: "Implement a concurrent file downloader. It should first send a HEAD request to get the file size, then divide the file into several segments. Each segment should be downloaded in a separate thread. Finally, assemble the downloaded segments into the final file.",
                inputFormat: "Implementation-based.",
                outputFormat: "A successfully downloaded and assembled file.",
                testCases: [{ input: "", output: "A large file is downloaded faster than a single-threaded download." }],
                solution: `// This is a complex task involving HTTP Range requests and file I/O.
// Conceptual solution:
public class ConcurrentDownloader {
    public void download(String url, String outputFile, int numThreads) throws Exception {
        // 1. Send HEAD request to get Content-Length.
        // 2. Calculate chunk size = contentLength / numThreads.
        // 3. Create an ExecutorService with numThreads.
        // 4. For each thread:
        //    - Calculate start and end byte range for its chunk.
        //    - Submit a task to the executor.
        //    - The task uses an HTTP client to make a GET request with a "Range: bytes=start-end" header.
        //    - The task writes the response body to a temporary file (e.g., file.part0, file.part1).
        // 5. Wait for all tasks to complete (executor.shutdown() and awaitTermination()).
        // 6. Assemble the part files into the final outputFile in the correct order.
    }
}`,
                explanation: "The key to this is the HTTP `Range` header, which allows a client to request only a specific portion of a file. The main thread acts as a coordinator, calculating the byte ranges for each thread. Each worker thread is responsible for downloading its assigned segment to a temporary file. After all workers have finished, the main thread concatenates these temporary files in the correct order to reconstruct the complete file."
            },
            {
                id: "java-s12-q79",
                title: "Timed Cache with Background Refresh",
                description: "Implement a cache where entries can be refreshed in the background.",
                statement: "Implement a cache where entries expire after a certain time. However, instead of just evicting, you can configure it to 'soft expire'. When a soft-expired item is requested, the cache returns the stale data immediately but triggers a background task to refresh the value asynchronously.",
                inputFormat: "Implementation-based.",
                outputFormat: "A cache that provides stale data while refreshing.",
                testCases: [{ input: "", output: "First request is slow, subsequent requests are fast (stale), then a later request is fast (fresh)." }],
                solution: `public class RefreshingCache<K, V> {
    // private final ConcurrentHashMap<K, CacheEntry<V>> cache;
    // private final ExecutorService refreshExecutor;
    // CacheEntry would store value, soft-expiry-time, and hard-expiry-time.

    public V get(K key) {
        // CacheEntry entry = cache.get(key);
        // if (entry is not expired) { return entry.getValue(); }
        //
        // if (entry is soft-expired) {
        //     // Return stale data
        //     // Trigger async refresh:
        //     // refreshExecutor.submit(() -> {
        //     //     V newValue = loadFromSource(key);
        //     //     put(key, newValue);
        //     // });
        //     return entry.getValue();
        // }
        
        // Hard-expired or not present, load synchronously
        // ...
        return null;
    }
}`,
                explanation: "This pattern, known as 'stale-while-revalidate', optimizes for availability. It avoids making the user wait for a slow data source to refresh. The cache entry needs to store two timestamps: a soft expiry time and a hard expiry time. If a request comes in after the soft expiry but before the hard expiry, the cache returns the old (stale) data but also starts a background thread to fetch the fresh data. Future requests will then receive the fresh data."
            },
            {
                id: "java-s12-q80",
                title: "Audit Trail Logger",
                description: "Implement an append-only logger with hash chaining.",
                statement: "Create an audit trail logger that writes to an append-only log file. To ensure integrity and detect tampering, each log entry should contain the hash of the previous log entry, forming a hash chain (a mini-blockchain).",
                inputFormat: "Implementation-based.",
                outputFormat: "A secure, append-only log file.",
                testCases: [{ input: "", output: "Each log line contains the hash of the line before it." }],
                solution: `import java.security.MessageDigest;

public class AuditLogger {
    private String lastHash = "00000000"; // Genesis hash

    public synchronized void log(String message) {
        String entry = System.currentTimeMillis() + "|" + lastHash + "|" + message;
        
        try {
            MessageDigest digest = MessageDigest.getInstance("SHA-256");
            byte[] hashBytes = digest.digest(entry.getBytes());
            String currentHash = bytesToHex(hashBytes);
            
            // Write the entry to the log file
            // writer.write(entry + "|" + currentHash + "\\n");
            
            this.lastHash = currentHash;
        } catch (Exception e) { /* ... */ }
    }
    private String bytesToHex(byte[] hash) { /* ... */ return "";}
}`,
                explanation: "This creates a tamper-evident log. Each log entry includes the hash of the previous entry. This creates a cryptographic chain. If an attacker modifies an entry in the middle of the log, its hash will change, which will break the chain because the subsequent entry's 'previous hash' field will no longer match. Verifying the integrity of the log involves recalculating the hash of each entry and ensuring it matches the 'previous hash' field of the next entry."
            },
            {
                id: "java-s12-q81",
                title: "Multi-Protocol Server",
                description: "Implement a server that handles HTTP and WebSocket on the same port.",
                statement: "This is a duplicate of problem #29 (WebSocket Server) and related to #14 (HTTP Server). A full implementation is complex.",
                inputFormat: "N/A",
                outputFormat: "N/A",
                testCases: [],
                solution: "// See WebSocket Server problem (#29) for the WebSocket part.\n// A real-world solution uses a framework like Netty or Vert.x that can handle protocol switching.",
                explanation: "Handling both protocols on one port requires parsing the initial HTTP request from a new connection. A standard request gets a standard HTTP response. A WebSocket connection begins as an HTTP GET request containing an `Upgrade: websocket` header. The server must detect this header, perform a specific handshake (involving hashing a key from the client), and then 'upgrade' the protocol for that connection from HTTP to WebSocket."
            },
            {
                id: "java-s12-q82",
                title: "Adaptive Sampling Profiler",
                description: "Implement a lightweight adaptive sampling profiler.",
                statement: "Implement a simple, lightweight profiler for a Java application. It should periodically take thread dumps of the target process, but adapt its sampling frequency. For example, it could sample more frequently when the CPU usage is high and less frequently when it is low.",
                inputFormat: "Implementation-based.",
                outputFormat: "A series of thread dumps that can be analyzed to find performance hotspots.",
                testCases: [{ input: "", output: "Profiler generates more samples during high CPU activity." }],
                solution: `// This is an advanced topic using JVMTI or process inspection.
// Conceptual solution:
public class AdaptiveProfiler {
    public void start(long targetProcessId) {
        new Thread(() -> {
            while (true) {
                // 1. Get current CPU usage of the target process.
                //    (This requires OS-specific APIs, e.g., via JNA or parsing 'top' output).
                
                // 2. Adjust sleep interval based on CPU.
                //    long sleepTime = (cpuUsage > 80) ? 10 : 100; // ms
                
                // 3. Take a thread dump of the target process.
                //    This is complex. One way is to run 'jstack <pid>' externally.
                //    Process p = new ProcessBuilder("jstack", String.valueOf(targetProcessId)).start();
                
                // 4. Sleep for the adjusted interval.
            }
        }).start();
    }
}`,
                explanation: "A sampling profiler works by taking snapshots (thread dumps) of the application's call stacks at regular intervals. The lines of code that appear most frequently in these snapshots are the 'hotspots' where the application is spending most of its time. An *adaptive* profiler adjusts its sampling rate based on system metrics like CPU usage to reduce its own overhead during idle periods while still capturing high-fidelity data during busy periods."
            },
            {
                id: "java-s12-q83",
                title: "DSL Parser with ANTLR",
                description: "Create a grammar and parse tree builder for a DSL.",
                statement: "Use the ANTLR (ANother Tool for Language Recognition) parser generator to create a parser for a simple Domain-Specific Language (DSL). Define a grammar for the DSL, generate the parser code with ANTLR, and use it in a Java program to build a parse tree from a DSL script.",
                inputFormat: "Implementation-based.",
                outputFormat: "A parse tree representing the input script.",
                testCases: [{ input: "", output: "The script is parsed successfully into a tree structure." }],
                solution: `// 1. Create a grammar file (e.g., MyDsl.g4)
// grammar MyDsl;
// expr: expr ('*'|'/') expr | expr ('+'|'-') expr | INT | ID;
//
// 2. Run ANTLR to generate Java parser classes.
//
// 3. Use the generated classes in Java:
// MyDslLexer lexer = new MyDslLexer(CharStreams.fromString("5 * 2 + 3"));
// CommonTokenStream tokens = new CommonTokenStream(lexer);
// MyDslParser parser = new MyDslParser(tokens);
// ParseTree tree = parser.expr(); // Start parsing at the 'expr' rule
//
// System.out.println(tree.toStringTree(parser)); // Print the tree`,
                explanation: "ANTLR is a powerful parser generator. You define the syntax of your language in a grammar file (`.g4`). ANTLR reads this grammar and generates Java classes (a lexer and a parser) that can recognize and parse your language. You then use these generated classes in your application to convert a string of code into a structured parse tree, which you can then walk to interpret or compile the code."
            },
            {
                id: "java-s12-q84",
                title: "Priority Inversion Scenario",
                description: "Simulate and fix a priority inversion scenario.",
                statement: "Create a scenario with three threads (High, Medium, Low priority) that demonstrates priority inversion. A low-priority thread holds a lock that a high-priority thread needs, but a medium-priority thread preempts the low-priority one, preventing it from releasing the lock. Then, show how this can be addressed (e.g., conceptually with priority inheritance).",
                inputFormat: "Implementation-based.",
                outputFormat: "Demonstration of the high-priority thread being starved.",
                testCases: [{ input: "", output: "High-priority thread waits while medium-priority thread runs." }],
                solution: `public class PriorityInversion {
    // A real fix requires OS-level support (Priority Inheritance Protocol)
    // or using priority-aware locks. Java's ReentrantLock can be configured with
    // a 'fairness' policy, which helps but doesn't solve this specific problem.
    
    // The simulation:
    // 1. Create a shared lock.
    // 2. Thread L (low prio) acquires the lock and does a long task.
    // 3. Thread H (high prio) starts and tries to acquire the same lock, blocking.
    // 4. Thread M (medium prio) starts. Since it has higher priority than L and doesn't need the lock,
    //    it preempts L and runs, starving H because L never gets CPU time to finish its task and release the lock.
}`,
                explanation: "Priority inversion is a dangerous scheduling problem. It occurs when a high-priority task is indirectly preempted by a lower-priority task, effectively 'inverting' their relative priorities. The classic example is the Mars Pathfinder mission failure. The solution is **priority inheritance**: when a high-priority thread blocks waiting for a lock held by a low-priority thread, the OS temporarily boosts the low-priority thread's priority to that of the waiting thread, allowing it to finish quickly and release the lock."
            },
            {
                id: "java-s12-q85",
                title: "Rolling-Window Aggregator",
                description: "Implement a real-time rolling-window aggregator.",
                statement: "This is a duplicate of problem #76 'Median Filter for Streaming Data', which is a specific type of rolling-window aggregator. Other aggregates like sum or average can also be implemented.",
                inputFormat: "N/A",
                outputFormat: "N/A",
                testCases: [],
                solution: "// See problem #76. The general approach uses a deque.",
                explanation: "A rolling-window aggregator processes a stream of data and maintains an aggregate (like sum, average, max) over the last N items or last T seconds. A `Deque` is an efficient data structure for this. As new items arrive, they are added to one end, and items that are no longer in the window are removed from the other end. The aggregate is updated incrementally with each add and remove operation."
            },
            {
                id: "java-s12-q86",
                title: "Custom Stream Collector",
                description: "Implement a custom collector that batches stream elements.",
                statement: "Implement a custom `java.util.stream.Collector`. The collector should take a stream of elements and group them into batches (lists) of a fixed size. For example, a stream of 10 items with a batch size of 3 would result in a `List<List<T>>` of `[[1,2,3], [4,5,6], [7,8,9], [10]]`.",
                inputFormat: "Implementation-based.",
                outputFormat: "A stream is collected into fixed-size batches.",
                testCases: [{ input: "", output: "Correctly batched lists are produced." }],
                solution: `import java.util.stream.Collector;
import java.util.stream.IntStream;
import java.util.ArrayList;

public class BatchCollector {
    public static <T> Collector<T, ?, List<List<T>>> toBatches(int batchSize) {
        return Collector.of(
            ArrayList::new, // Supplier
            (list, item) -> { // Accumulator
                if (list.isEmpty() || list.get(list.size() - 1).size() == batchSize) {
                    list.add(new ArrayList<>());
                }
                list.get(list.size() - 1).add(item);
            },
            (list1, list2) -> { /* Combiner for parallel streams */ list1.addAll(list2); return list1; }
        );
    }
    public static void main(String[] args) {
        List<List<Integer>> batches = IntStream.range(0, 10).boxed()
                                           .collect(toBatches(3));
        System.out.println(batches); // [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
    }
}`,
                explanation: "A custom `Collector` is defined by four functions: a `supplier` (creates the result container), an `accumulator` (adds an element to the container), a `combiner` (merges containers in parallel streams), and an optional `finisher`. Our accumulator checks if the current last batch is full; if so, it creates a new batch before adding the item. This logic groups the stream elements into the desired batch sizes."
            },
            {
                id: "java-s12-q87",
                title: "JAR Comparison Tool",
                description: "Compare two JARs and list API changes.",
                statement: "Create a tool that can compare two versions of a JAR file and report on the public API changes between them. It should list added/removed classes and added/removed/modified public methods and fields.",
                inputFormat: "Implementation-based.",
                outputFormat: "A report detailing API changes.",
                testCases: [{ input: "", output: "Correctly identifies a newly added public method." }],
                solution: `// This is a complex task requiring bytecode inspection libraries like ASM or Javassist.
// Conceptual approach:
public class JarComparer {
    public void compare(String jar1Path, String jar2Path) {
        // 1. Use JarFile to iterate through entries of both JARs.
        // 2. Identify all ".class" files in each.
        // 3. Find classes present in jar1 but not jar2 (Removed), and vice-versa (Added).
        // 4. For common classes:
        //    a. Use a bytecode library (like ASM's ClassReader) to parse the class file.
        //    b. Get a list of all public methods and fields for the class in jar1.
        //    c. Get a list for the class in jar2.
        //    d. Compare the lists to find added/removed/modified signatures.
    }
}`,
                explanation: "This requires reading the contents of JAR files and parsing the Java bytecode (`.class` files) within them. Bytecode manipulation libraries like ASM are essential. The tool would load all class structures from both JARs into memory. It would then compare these structures, first at the class level to find added/removed classes, and then for common classes, it would compare their method and field signatures to identify changes."
            },
            {
                id: "java-s12-q88",
                title: "Secure Token-Based Authentication",
                description: "Implement a JWT-like system with refresh tokens.",
                statement: "Implement a secure token-based authentication system. A login endpoint should return a short-lived access token (e.g., JWT, 15 minutes) and a long-lived refresh token. Endpoints should be protected by the access token. A `/refresh` endpoint should accept a valid refresh token and issue a new access token.",
                inputFormat: "Implementation-based.",
                outputFormat: "A secure authentication flow.",
                testCases: [{ input: "", output: "Expired access token is rejected, but new one can be obtained with refresh token." }],
                solution: `// Requires a JWT library like jjwt or auth0's java-jwt
public class TokenService {
    // public String generateAccessToken(String username) {
    //     return Jwts.builder()
    //         .setSubject(username)
    //         .setExpiration(new Date(System.currentTimeMillis() + 15 * 60 * 1000))
    //         .signWith(SECRET_KEY)
    //         .compact();
    // }
    // public String generateRefreshToken(String username) { /* ... longer expiry ... */ }
    
    // public boolean validateToken(String token) { /* ... try to parse with key ... */ }
}`,
                explanation: "This is the standard OAuth2/JWT flow. The **access token** is used to access protected resources but has a short expiry time to limit the damage if it's stolen. The **refresh token** has a long expiry but can only be used at a specific `/refresh` endpoint to get a new access token. This avoids forcing the user to log in again every 15 minutes. The refresh token must be stored securely (e.g., an HttpOnly cookie)."
            },
            {
                id: "java-s12-q89",
                title: "Polite Web Crawler",
                description: "Build a crawler with politeness, rate-limiting, and robots.txt.",
                statement: "This is a duplicate of problem #78's concepts (Concurrent Downloader) but with added politeness rules.",
                inputFormat: "Implementation-based.",
                outputFormat: "A web crawler that respects website rules.",
                testCases: [{ input: "", output: "Crawler does not access disallowed paths and respects delays." }],
                solution: `// This combines several concepts:
// 1. Concurrency: Use an ExecutorService and HttpClient for async requests (#28).
// 2. Rate Limiting: Before making a request to a domain, check a map of last-request-times
//    and sleep if the politeness delay has not passed.
// 3. robots.txt: Before crawling any domain for the first time, fetch and parse its
//    /robots.txt file. Use a library to parse it. Before fetching any URL,
//    check if your user-agent is allowed to access that path.`,
                explanation: "A responsible scraper is a polite one. **`robots.txt`** is a standard file websites use to tell bots which pages they are not allowed to crawl. Your scraper must fetch and respect this file. **Politeness delay** means waiting for a short period (e.g., 1-2 seconds) between requests to the same server to avoid overloading it. **Concurrency** is used to work on other domains while waiting for the politeness delay on one domain to expire."
            },
            {
                id: "java-s12-q90",
                title: "Concurrent BitSet",
                description: "Implement a thread-safe bitset with atomic operations.",
                statement: "Implement a thread-safe `BitSet`. While `java.util.BitSet`'s individual methods are synchronized, compound operations are not. Create a version or a wrapper that supports atomic operations like `setAndGetPrevious`.",
                inputFormat: "Implementation-based.",
                outputFormat: "A thread-safe bitset.",
                testCases: [{ input: "", output: "Concurrent operations do not corrupt the bitset's state." }],
                solution: `import java.util.concurrent.atomic.AtomicLongArray;

public class ConcurrentBitSet {
    private final AtomicLongArray bits;

    public ConcurrentBitSet(int size) {
        int longCount = (size + 63) / 64;
        this.bits = new AtomicLongArray(longCount);
    }
    
    public void set(int bitIndex) {
        int longIndex = bitIndex / 64;
        long bitMask = 1L << (bitIndex % 64);
        
        long oldValue;
        do {
            oldValue = bits.get(longIndex);
        } while (!bits.compareAndSet(longIndex, oldValue, oldValue | bitMask));
    }
    
    public boolean get(int bitIndex) {
        int longIndex = bitIndex / 64;
        long bitMask = 1L << (bitIndex % 64);
        return (bits.get(longIndex) & bitMask) != 0;
    }
}`,
                explanation: "A highly concurrent `BitSet` can be implemented using an `AtomicLongArray`. Each `long` in the array represents 64 bits. To set a specific bit, we identify which `long` it belongs to and create a bitmask. We then use a `compareAndSet` (CAS) loop to atomically update the `long` by OR-ing it with the bitmask. This lock-free approach is very efficient for high-contention scenarios."
            },
            {
                id: "java-s12-q91",
                title: "Sandboxed Code Execution",
                description: "Create a sandboxed environment for running untrusted code.",
                statement: "Create a sandboxed environment to execute untrusted Java code. The sandbox should prevent the code from accessing the filesystem, network, or starting new threads. This involves using a custom `SecurityManager` and a `Policy` file.",
                inputFormat: "Implementation-based.",
                outputFormat: "Untrusted code runs in a restricted environment.",
                testCases: [{ input: "Code tries to read a file", output: "AccessControlException is thrown." }],
                solution: `// This is a deprecated feature in modern Java (since Java 17).
// The modern approach is to run untrusted code in a separate process or container (e.g., Docker).
// Conceptual pre-Java 17 solution:
//
// 1. Create a policy file (e.g., my.policy)
//    grant {
//        // No permissions granted
//    };
//
// 2. Set the SecurityManager and policy property when running
//    java -Djava.security.manager -Djava.security.policy==my.policy MyApp
//
// 3. In code, run the untrusted part using AccessController.doPrivileged
//    or simply let the SecurityManager enforce the rules.
`,
                explanation: "Java's `SecurityManager` was the traditional way to create a sandbox. By defining a strict security policy file, you could deny permissions for dangerous operations like file I/O or network access. However, the `SecurityManager` has been deprecated for removal because it is complex and difficult to use correctly. The modern, more secure approach is to use OS-level sandboxing by running untrusted code in a separate process with restricted permissions or inside a container."
            },
            {
                id: "java-s12-q92",
                title: "Two-Phase Commit Simulator",
                description: "Simulate a two-phase commit protocol for distributed transactions.",
                statement: "Simulate a Two-Phase Commit (2PC) protocol. Create a Transaction Coordinator and several Participants (resources). The coordinator should manage the two phases: 1) The 'Prepare' phase where it asks all participants if they can commit, and 2) The 'Commit' phase where it instructs them to commit or abort based on the votes.",
                inputFormat: "Implementation-based.",
                outputFormat: "A transaction either commits across all participants or aborts on all.",
                testCases: [{ input: "One participant fails to prepare", output: "All participants receive an abort command." }],
                solution: `// Conceptual implementation
public class Coordinator {
    // List<Participant> participants;
    
    public boolean executeTransaction() {
        // Phase 1: Prepare (Voting)
        boolean allPrepared = true;
        for (Participant p : participants) {
            if (!p.prepare()) {
                allPrepared = false;
                break;
            }
        }
        
        // Phase 2: Commit / Abort
        if (allPrepared) {
            System.out.println("All prepared. Committing...");
            for (Participant p : participants) p.commit();
            return true;
        } else {
            System.out.println("One or more failed to prepare. Aborting...");
            for (Participant p : participants) p.abort();
            return false;
        }
    }
}`,
                explanation: "2PC is a protocol for achieving atomic transactions across multiple distributed resources. In the **Prepare Phase**, the coordinator asks each participant to get ready to commit. Each participant prepares its changes and votes 'yes' or 'no'. In the **Commit Phase**, if all participants voted 'yes', the coordinator tells them all to commit. If even one voted 'no' (or timed out), the coordinator tells them all to abort (rollback). This ensures the transaction is all-or-nothing."
            },
            {
                id: "java-s12-q93",
                title: "High-Performance CSV Exporter",
                description: "Build a CSV exporter that streams data without high memory usage.",
                statement: "Create a high-performance CSV exporter that can write millions of records from a database query (`ResultSet`) to a file. The implementation must be streaming, meaning it should not load all the data into memory at once.",
                inputFormat: "Implementation-based.",
                outputFormat: "A large CSV file is generated with low memory footprint.",
                testCases: [{ input: "", output: "Successfully exports a large dataset." }],
                solution: `import java.sql.ResultSet;
import java.io.Writer;

public class CsvExporter {
    public void export(ResultSet rs, Writer writer) throws Exception {
        // 1. Get metadata to write header
        int columnCount = rs.getMetaData().getColumnCount();
        for (int i = 1; i <= columnCount; i++) {
            writer.write(rs.getMetaData().getColumnName(i));
            if (i < columnCount) writer.write(",");
        }
        writer.write("\\n");
        
        // 2. Stream rows
        while (rs.next()) {
            for (int i = 1; i <= columnCount; i++) {
                // Need to handle commas and quotes within values
                writer.write(escapeCsv(rs.getString(i)));
                if (i < columnCount) writer.write(",");
            }
            writer.write("\\n");
        }
    }
    
    private String escapeCsv(String value) { /* ... */ return "";}
}`,
                explanation: "The key to streaming is processing the `ResultSet` row by row in a `while (rs.next())` loop. Inside the loop, we format the current row into a CSV string and immediately write it to the output file using a `Writer`. This ensures that only one row of data is held in memory at any given time, allowing the exporter to handle datasets of virtually any size with a constant, low memory footprint."
            },
            {
                id: "java-s12-q94",
                title: "Prioritized Message Queue",
                description: "Implement a message queue with delayed messages.",
                statement: "Implement a message queue system that supports message priorities and delayed delivery. A producer should be able to submit a message with a priority and a delay. Consumers should always receive the highest-priority available message. Messages should only become visible to consumers after their delay has passed.",
                inputFormat: "Implementation-based.",
                outputFormat: "A queue that respects priority and delay.",
                testCases: [{ input: "", output: "High-priority message is consumed before low-priority, delayed message is consumed last." }],
                solution: `import java.util.concurrent.DelayQueue;
import java.util.concurrent.Delayed;
import java.util.concurrent.TimeUnit;

public class PrioritizedDelayedQueue {
    // We can combine a DelayQueue with a custom object that holds priority.
    // However, DelayQueue itself doesn't prioritize.
    // A better approach is a PriorityBlockingQueue with a custom Delayed object.

    static class PrioritizedDelayedTask implements Delayed, Comparable<PrioritizedDelayedTask> {
        private final int priority;
        private final long executeTime;
        // ...
        
        @Override
        public int compareTo(PrioritizedDelayedTask other) {
            // Compare first by executeTime, then by priority
            if (this.executeTime < other.executeTime) return -1;
            if (this.executeTime > other.executeTime) return 1;
            return Integer.compare(this.priority, other.priority);
        }
    }
}`,
                explanation: "This is a complex queueing problem. A good solution would use a `PriorityBlockingQueue` as the underlying data structure. The items placed in the queue would be custom objects that implement both `Delayed` (for the delay functionality) and `Comparable`. The `compareTo` method would be implemented to first compare items based on their execution time, and for items with the same time, compare them based on their priority. This ensures a consumer calling `take()` will always get the highest-priority item whose delay has expired."
            },
            {
                id: "java-s12-q95",
                title: "Image Processing Pipeline",
                description: "Build a parallel image processing pipeline.",
                statement: "Create an image processing pipeline that can take a batch of images and apply a series of transformations (e.g., resize, watermark, compress) in parallel. Use an `ExecutorService` to manage the workload.",
                inputFormat: "Implementation-based.",
                outputFormat: "Processed images are saved to an output directory.",
                testCases: [{ input: "", output: "A batch of images is processed concurrently." }],
                solution: `import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
// Assumes a library like Thumbnailator or Java's ImageIO
// import net.coobird.thumbnailator.Thumbnails;

public class ImagePipeline {
    public void processBatch(List<File> images) {
        ExecutorService executor = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors());
        
        for (File imageFile : images) {
            executor.submit(() -> {
                try {
                    // Thumbnails.of(imageFile)
                    //     .size(800, 600)
                    //     .watermark(...)
                    //     .outputQuality(0.8)
                    //     .toFile(new File("output/" + imageFile.getName()));
                } catch (Exception e) {
                    e.printStackTrace();
                }
            });
        }
        
        executor.shutdown();
    }
}`,
                explanation: "Image processing is often a CPU-intensive task that benefits greatly from parallelization. This solution uses a fixed-size thread pool, sized to the number of available CPU cores for optimal performance. Each image processing task is submitted as a separate `Runnable` to the `ExecutorService`. The executor manages running these tasks concurrently on the available threads, significantly speeding up the processing of a large batch of images."
            },
            {
                id: "java-s12-q96",
                title: "K-Means Clustering",
                description: "Implement the K-Means clustering algorithm.",
                statement: "Implement the K-Means clustering algorithm from scratch. The function should take a list of data points (e.g., 2D points) and the number of clusters (K) as input. It should iteratively assign points to the nearest centroid and update the centroids until convergence.",
                inputFormat: "Implementation-based.",
                outputFormat: "The final cluster assignments for each point.",
                testCases: [{ input: "", output: "Data points are grouped into K clusters." }],
                solution: `// This is a complex algorithm to implement from scratch.
public class KMeans {
    public int[] cluster(double[][] data, int k) {
        // 1. Initialize K centroids randomly.
        // 2. Loop until convergence:
        //    a. Assignment Step: For each data point, find the closest centroid
        //       (using Euclidean distance) and assign the point to that cluster.
        //    b. Update Step: For each cluster, recalculate its centroid by taking
        //       the mean of all points assigned to it.
        //    c. Check for convergence (e.g., if centroids have stopped moving).
        // 3. Return the final cluster assignments.
        return new int[0];
    }
}`,
                explanation: "K-Means is an iterative algorithm for partitioning data into K clusters. It starts with random cluster centers (centroids). In the **assignment step**, each data point is assigned to its nearest centroid. In the **update step**, the centroids are moved to the center (mean) of their assigned points. These two steps are repeated until the cluster assignments no longer change, indicating that the algorithm has converged."
            },
            {
                id: "java-s12-q97",
                title: "Data Deduplication Utility",
                description: "Find duplicate files using chunking and content-based hashing.",
                statement: "Write an efficient tool to find duplicate files in a directory. It should first identify files of the same size, then compare a small hash of the first few KB, and only if those match, compute and compare the full file hash (e.g., SHA-256) to avoid reading large files unnecessarily.",
                inputFormat: "A directory path.",
                outputFormat: "A list of groups of duplicate file paths.",
                testCases: [{ input: "", output: "Identifies duplicate files correctly and efficiently." }],
                solution: `public class FileDeduplicator {
    public void findDuplicates(String dirPath) {
        // 1. First pass: Group files by size.
        //    Map<Long, List<File>> filesBySize = ...
        //    Iterate through all files, populate this map.

        // 2. Second pass: For groups with >1 file, hash a small chunk.
        //    for (List<File> group : filesBySize.values()) {
        //        Map<String, List<File>> filesByPartialHash = ...
        //        // For each file in group, read first 4KB, hash it, populate map.
        //    }
        
        // 3. Final pass: For sub-groups with >1 file, do a full hash.
        //    // For each file in sub-group, read the whole file, SHA-256 hash it.
        //    // Any files with the same full hash are duplicates.
    }
}`,
                explanation: "This multi-stage approach is an optimization to avoid the expensive operation of reading and hashing large files. Grouping by file size is a very fast initial filter, as files of different sizes cannot be duplicates. Hashing a small, initial chunk is the next fastest filter. Only the very few files that match in both size and initial hash need to have their full contents read and hashed, making the process efficient for large directories."
            },
            {
                id: "java-s12-q98",
                title: "Bounded Blocking Deque",
                description: "Implement a thread-safe, bounded, blocking double-ended queue.",
                statement: "Implement a bounded (fixed-size) blocking deque. It should support `putFirst`/`putLast` (which block if full) and `takeFirst`/`takeLast` (which block if empty). Use a single `ReentrantLock` and two `Condition` variables (one for 'notFull', one for 'notEmpty').",
                inputFormat: "Implementation-based.",
                outputFormat: "A thread-safe, blocking deque.",
                testCases: [{ input: "", output: "Concurrent producers and consumers at both ends work correctly." }],
                solution: `import java.util.concurrent.locks.Condition;
import java.util.concurrent.locks.ReentrantLock;

public class BoundedBlockingDeque<T> {
    private final T[] items;
    private final ReentrantLock lock = new ReentrantLock();
    private final Condition notFull = lock.newCondition();
    private final Condition notEmpty = lock.newCondition();
    // ... indices for head, tail, and count ...

    public void putLast(T item) throws InterruptedException {
        lock.lock();
        try {
            while (count == items.length) notFull.await(); // Wait until not full
            // ... add item to tail ...
            notEmpty.signal(); // Signal that it's no longer empty
        } finally { lock.unlock(); }
    }
    
    public T takeFirst() throws InterruptedException {
        lock.lock();
        try {
            while (count == 0) notEmpty.await(); // Wait until not empty
            // ... take item from head ...
            notFull.signal(); // Signal that it's no longer full
            // return item;
        } finally { lock.unlock(); }
        return null;
    }
}`,
                explanation: "This is similar to the circular buffer but more advanced. A single lock protects the internal array. Two `Condition` variables are used for more efficient signaling. `notFull` is used to wait for space to become available, and `notEmpty` is used to wait for an item to be present. Using two conditions prevents 'thundering herd' problems where a `notifyAll` wakes up both waiting producers and consumers when only one type can proceed."
            },
            {
                id: "java-s12-q99",
                title: "Dynamic Proxy AOP Logger",
                description: "Use a dynamic proxy to log method calls.",
                statement: "Use Java's `java.lang.reflect.Proxy` to create a dynamic proxy for an interface. The proxy's invocation handler should log the entry, exit, and execution time of any method called on the proxied object, demonstrating a simple Aspect-Oriented Programming (AOP) concept.",
                inputFormat: "Implementation-based.",
                outputFormat: "Method calls are automatically logged with their execution times.",
                testCases: [{ input: "", output: "Calling method 'doWork'... Exiting method 'doWork', duration: 100ms." }],
                solution: `import java.lang.reflect.InvocationHandler;
import java.lang.reflect.Method;
import java.lang.reflect.Proxy;

interface MyService { void doWork(); }
class MyServiceImpl implements MyService {
    public void doWork() { /* ... does work ... */ }
}

class LoggingInvocationHandler implements InvocationHandler {
    private final Object target;
    public LoggingInvocationHandler(Object target) { this.target = target; }

    @Override
    public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {
        System.out.println("Calling method " + method.getName() + "...");
        long start = System.nanoTime();
        Object result = method.invoke(target, args); // Call original method
        long elapsed = System.nanoTime() - start;
        System.out.println("Exiting method " + method.getName() + ", duration: " + elapsed / 1e6 + "ms.");
        return result;
    }
}
// To use:
// MyService service = new MyServiceImpl();
// MyService proxiedService = (MyService) Proxy.newProxyInstance(
//     MyService.class.getClassLoader(), new Class<?>[]{MyService.class}, new LoggingInvocationHandler(service));
// proxiedService.doWork(); // This will be logged`,
                explanation: "A dynamic proxy is an object that is created at runtime and implements a given list of interfaces. All method calls on the proxy are dispatched to a single `invoke` method in its `InvocationHandler`. This handler can then add behavior before or after calling the original method (`method.invoke(target, ...)`), which is the essence of AOP. It allows you to add cross-cutting concerns like logging or transactions without modifying the original business logic."
            },
            {
                id: "java-s12-q100",
                title: "Suffix Array Construction",
                description: "Implement an efficient suffix array construction algorithm.",
                statement: "Implement an efficient O(n log^2 n) or O(n log n) algorithm to construct a suffix array for a given string. A suffix array is a sorted array of all suffixes of a string, and it is a powerful data structure for various string problems.",
                inputFormat: "A single string.",
                outputFormat: "An array of integers representing the starting indices of the sorted suffixes.",
                testCases: [{ input: "'banana'", output: "[5, 3, 1, 0, 4, 2]" }],
                solution: `// O(n log^2 n) implementation is complex for a snippet.
// A simpler O(n^2 log n) version for understanding:
public class SuffixArraySimple {
    public static int[] build(String s) {
        int n = s.length();
        String[] suffixes = new String[n];
        for (int i = 0; i < n; i++) {
            suffixes[i] = s.substring(i);
        }
        
        // Sort the suffixes lexicographically
        Arrays.sort(suffixes);
        
        // Create the suffix array (indices)
        int[] suffixArr = new int[n];
        Map<String, Integer> originalIndices = new HashMap<>();
        for (int i = 0; i < n; i++) originalIndices.put(s.substring(i), i);
        
        for (int i = 0; i < n; i++) {
            suffixArr[i] = originalIndices.get(suffixes[i]);
        }
        
        return suffixArr;
    }
}`,
                explanation: "A suffix array stores the starting indices of all suffixes of a string in lexicographical (alphabetical) order. For 'banana', the suffixes are 'banana', 'anana', 'nana', 'ana', 'na', 'a'. Sorting these gives: 'a', 'ana', 'anana', 'banana', 'na', 'nana'. Their original starting indices are `[5, 3, 1, 0, 4, 2]`, which is the suffix array. The naive sorting approach is slow. Efficient algorithms work by sorting suffixes based on their first 2 characters, then 4, then 8, and so on, doubling the comparison length in each step."
            }
        ]
    }
];